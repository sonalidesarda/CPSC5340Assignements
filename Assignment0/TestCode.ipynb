{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  This cell is a solution and for your reference.\n",
    "###              So when grading, delete this cell, you will use only the tests below.\n",
    "\n",
    "import os\n",
    "def getIDsAndReviews(dataDir):\n",
    "    dict = {}\n",
    "    for filename in os.listdir(dataDir):\n",
    "        dict[getDocID(filename)] = readFile(dataDir, filename)\n",
    "    return dict\n",
    "        \n",
    "def getDocID(filename):\n",
    "    return os.path.splitext(os.path.basename(filename))[0]\n",
    "\n",
    "def readFile(dataDir, filename):\n",
    "    return open(f'{dataDir}/{filename}').read()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#  This should return a list of strings, every string should be all lower-case letters\n",
    "def cleanReview(review):\n",
    "    return list(filter(lambda s: s.isalpha(), [s.lower() for s in word_tokenize(review)]))\n",
    "\n",
    "def cleanReviews(idToReviews):\n",
    "    for docID, review in idToReviews.items():\n",
    "        idToReviews[docID] = cleanReview(review)\n",
    "\n",
    "import re\n",
    "def tokenizeWord(word):\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    return regex.sub('', word).lower()\n",
    "\n",
    "#   Tokenize a single review (string)\n",
    "#  Input -- a review (string)\n",
    "#  Output -- the cleaned review -- a list of words (strings), each word is cleaned\n",
    "def tokenizeDocument(document):\n",
    "    tokens = [tokenizeWord(word) for word in document.split()]\n",
    "    return [tok for tok in tokens if len(tok) > 0]\n",
    "\n",
    "# Input -- a dictionary as produced by getIDsAndDocuments\n",
    "# Output -- a dictionary with the same keys, but each value is the cleaned review (a list of strings)\n",
    "def tokenizeDocuments(idToDocuments):\n",
    "    dict = {}\n",
    "    for docID, document in idToDocuments.items():\n",
    "        dict[docID] = tokenizeDocument(document)\n",
    "    return dict\n",
    "\n",
    "def buildTermToDocDictionaries(docsToTerms):\n",
    "    dd = []\n",
    "    for docID, terms in docsToTerms.items():\n",
    "        dd.append(dict(list(zip(terms, [{docID} for n in range(0, len(terms))]))))\n",
    "    return dd\n",
    "\n",
    "def mergeTwo(d1, d2):\n",
    "    d = {}\n",
    "    allTerms = set(d1.keys()) | set(d2.keys())\n",
    "    for term in allTerms:\n",
    "        s1 = d1.get(term, set())\n",
    "        s2 = d2.get(term, set())\n",
    "        d[term] = s1 | s2\n",
    "    return d\n",
    "\n",
    "def mergeIndexes(indexes):\n",
    "    dout = {}\n",
    "    for d in indexes:\n",
    "        dout = mergeTwo(dout, d)\n",
    "    return dout\n",
    "\n",
    "def indexReviews(dataDir):\n",
    "    return mergeIndexes(buildTermToDocDictionaries(tokenizeDocuments(getIDsAndReviews(dataDir))))\n",
    "\n",
    "#######################################\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, dataDir):\n",
    "        self.index = indexReviews(dataDir)\n",
    "    def query(self, term):\n",
    "        return self.index.get(term, set())\n",
    "    def andQuery(self, term1, term2):\n",
    "        t1 = term1 if type(term1) == set else self.query(term1)\n",
    "        t2 = term2 if type(term2) == set else self.query(term2)\n",
    "        return t1 & t2\n",
    "    def orQuery(self, term1, term2):\n",
    "        t1 = term1 if type(term1) == set else self.query(term1)\n",
    "        t2 = term2 if type(term2) == set else self.query(term2)\n",
    "        return t1 | t2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1: Succeeded\n",
      "q2: Succeeded\n",
      "q3: Succeeded\n",
      "q4: Succeeded\n",
      "q5: Succeeded\n",
      "q6: Succeeded\n",
      "q7: Succeeded\n",
      "\n",
      "==========================================\n",
      "\n",
      "q1: Succeeded\n",
      "q2: Succeeded\n",
      "q3: Succeeded\n",
      "q4: Succeeded\n",
      "q5: Succeeded\n",
      "q6: Succeeded\n",
      "q7: Succeeded\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "##  For testing / grading.  Copy this cell at the end of the student's solution\n",
    "##  then restart kernel and run all\n",
    "\n",
    "def testEqual(msg, expected, actual):\n",
    "    res = None\n",
    "    if expected == actual:\n",
    "        res = \"Succeeded\"\n",
    "    else:\n",
    "        res = f\"Failed, expected {expected} got {actual}\"\n",
    "    print(f\"{msg}: {res}\")\n",
    "\n",
    "def testOriginal():\n",
    "    qp = QueryProcessor('data')\n",
    "    q1 = sorted(qp.query('movie'))\n",
    "    testEqual(\"q1\", ['Dog1', 'Dog2', 'Dog3', 'Dog4', 'Joe1', 'Joe2'], q1)\n",
    "    q2 = sorted(qp.andQuery('yellow', 'lab'))\n",
    "    testEqual(\"q2\", ['Dog1', 'Dog2'], q2)\n",
    "    q3 = sorted(qp.andQuery(qp.andQuery('salome', qp.orQuery('good', 'excellent')), 'opera'))\n",
    "    testEqual(\"q3\", ['Salome5'], q3)\n",
    "    q4 = sorted(qp.orQuery(qp.orQuery('good', 'excellent'), qp.orQuery('liked', 'loved')))\n",
    "    testEqual(\"q4\", ['Dog4', 'DooWop1', 'DooWop2', 'Joe1', 'Joe2', 'Salome5'], q4)\n",
    "    q5 = list(qp.query(\"xxx\"))\n",
    "    testEqual(\"q5\", [], q5)\n",
    "    q6 = sorted(qp.orQuery(\"xxx\", 'movie'))\n",
    "    testEqual(\"q6\", ['Dog1', 'Dog2', 'Dog3', 'Dog4', 'Joe1', 'Joe2'], q6)\n",
    "    q7 = list(qp.andQuery(\"xxx\", 'movie'))\n",
    "    testEqual(\"q7\", [], q7)\n",
    "    \n",
    "def testBooks():\n",
    "    DATA_DIR = 'textcorpora'\n",
    "    qp = QueryProcessor('textcorpora')\n",
    "    q1 = sorted(qp.query('king'))\n",
    "    testEqual(\"q1\", ['bible-kjv', 'melville-moby_dick', 'shakespeare-macbeth'], q1)    \n",
    "    q2 = sorted(qp.andQuery('happy', 'death'))\n",
    "    testEqual(\"q2\", ['bible-kjv', 'melville-moby_dick', 'shakespeare-macbeth'], q2)  \n",
    "    q3 = sorted(qp.andQuery(qp.andQuery('whale', qp.orQuery('good', 'excellent')), 'ocean'))\n",
    "    testEqual(\"q3\", ['melville-moby_dick'], q3)\n",
    "    q4 = sorted(qp.orQuery(qp.orQuery('good', 'excellent'), qp.orQuery('liked', 'loved')))\n",
    "    testEqual(\"q4\", ['bible-kjv', 'melville-moby_dick', 'shakespeare-macbeth'], q4)\n",
    "    q5 = sorted(qp.query(\"xxx\"))\n",
    "    testEqual(\"q5\", [], q5)\n",
    "    q6 = sorted(qp.orQuery(\"xxx\", 'mother'))\n",
    "    testEqual(\"q6\", ['bible-kjv', 'melville-moby_dick', 'shakespeare-macbeth'], q6)\n",
    "    q7 = sorted(qp.andQuery(\"xxx\", 'movie'))\n",
    "    testEqual(\"q7\", [], q7)\n",
    "    \n",
    "def doTests():\n",
    "    testOriginal()\n",
    "    print(\"\\n==========================================\\n\")\n",
    "    testBooks()\n",
    "    \n",
    "doTests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
