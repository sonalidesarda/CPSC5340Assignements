{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name : Sonali Pravin Desarda \n",
    "# Course Number : CPSC5340 01 22WQ\n",
    "# Assignment 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def getDocID(filename):\n",
    "    return os.path.splitext(os.path.basename(filename))[0]\n",
    "\n",
    "def getIDsAndReviews(dataDir):\n",
    "    path = os.path.abspath(dataDir)\n",
    "    textDict = {}\n",
    "    with os.scandir(path) as dir:\n",
    "        for entry in dir:\n",
    "            if entry.is_file():\n",
    "                docID = getDocID(entry.name)\n",
    "                review_text = open(entry.path).read()\n",
    "                textDict[docID] = review_text\n",
    "    return textDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenizeWord(word):\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    return regex.sub('', word).lower()\n",
    "\n",
    "def tokenizeDocument(document):\n",
    "    words = document.split()\n",
    "    tokenize_words = []\n",
    "    for word in words:\n",
    "        tokenize_word = tokenizeWord(word)\n",
    "        tokenize_words.append(tokenize_word)\n",
    "    return tokenize_words\n",
    "\n",
    "def tokenizeDocuments(idToDocuments):\n",
    "    tokenizeDocumentDict = {}\n",
    "    for docID in idsAndReviews.keys():\n",
    "        review_list = tokenizeDocument(idToDocuments[docID])\n",
    "        tokenizeDocumentDict[docID] = review_list\n",
    "    return tokenizeDocumentDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTermToDocDictionaries(docsToTerms):\n",
    "    indexes_list = []\n",
    "    for docID in docsToTerms.keys():\n",
    "        wordDict = {}\n",
    "        review_list = docsToTerms[docID]\n",
    "        for token in review_list:\n",
    "            wordDict[token] = set()\n",
    "            wordDict[token].add(docID)\n",
    "        indexes_list.append(wordDict)\n",
    "    return indexes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeIndexes(indexes):\n",
    "    dataDir = dict()\n",
    "    for index in indexes:\n",
    "        for term in index.keys():\n",
    "            docID = index[term]\n",
    "            if term not in dataDir:\n",
    "                dataDir[term] = set()\n",
    "            dataDir[term].update(docID)\n",
    "    return dataDir    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexReviews(dataDir):\n",
    "    idsAndReviews = getIDsAndReviews(dataDir)\n",
    "    tokenizeDocumentDict = tokenizeDocuments(idsAndReviews)\n",
    "    indexes = buildTermToDocDictionaries(tokenizeDocumentDict)\n",
    "    inverted_indexes = mergeIndexes(indexes)\n",
    "    return inverted_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    def __init__(self, dataDir):\n",
    "        invertedIndexes = indexReviews(dataDir)\n",
    "        self.inverted_indexes = invertedIndexes\n",
    "        \n",
    "    def query(self, term):\n",
    "        docIds = set()\n",
    "        if term in self.inverted_indexes:\n",
    "            docIds.update(self.inverted_indexes[term])\n",
    "        return docIds\n",
    "        \n",
    "    def andQuery(self, term1, term2):\n",
    "        docIds = set()\n",
    "        term1_set = set()\n",
    "        term2_set = set()\n",
    "        \n",
    "        if isinstance(term1,str):\n",
    "            term1_set = self.query(term1)\n",
    "        else:\n",
    "            term1_set = term1\n",
    "        \n",
    "        if isinstance(term2,str):\n",
    "            term2_set = self.query(term2)\n",
    "        else:\n",
    "            term2_set = term2\n",
    "                \n",
    "        docIds = term1_set & term2_set\n",
    "        return docIds\n",
    "        \n",
    "    def orQuery(self, term1, term2):\n",
    "        docIds = set()\n",
    "        term1_set = set()\n",
    "        term2_set = set()\n",
    "        \n",
    "        if isinstance(term1,str):\n",
    "            term1_set = self.query(term1)\n",
    "        else:\n",
    "            term1_set = term1\n",
    "        \n",
    "        if isinstance(term2,str):\n",
    "            term2_set = self.query(term2)\n",
    "        else:\n",
    "            term2_set = term2\n",
    "                \n",
    "        docIds = term1_set | term2_set\n",
    "        return docIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dog4', 'Dog1', 'Dog2', 'Joe1', 'Joe2', 'Dog3'}\n",
      "{'Dog1', 'Dog2'}\n",
      "{'Salome5'}\n",
      "{'Dog4', 'Joe1', 'DooWop2', 'DooWop1', 'Salome5', 'Joe2'}\n",
      "{'DooWop2'}\n",
      "{'Dog4', 'Salome5', 'Joe1', 'DooWop2'}\n",
      "{'DooWop1', 'Joe2', 'DooWop2'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "qp = QueryProcessor(\"data\")\n",
    "\n",
    "print(qp.query('movie'))\n",
    "print(qp.andQuery('yellow', 'lab'))\n",
    "print(qp.andQuery(qp.andQuery('salome', qp.orQuery('good', 'excellent')), 'opera'))\n",
    "print(qp.orQuery(qp.orQuery('good', 'excellent'), qp.orQuery('liked', 'loved')))\n",
    "print(qp.andQuery(qp.andQuery('good', 'excellent'), qp.orQuery('good', 'loved')))\n",
    "print(qp.query('good'))\n",
    "print(qp.query('excellent'))\n",
    "print(qp.query('loved'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
