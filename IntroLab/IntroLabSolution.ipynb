{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IntroLab -- TCSS 5340 -- SOLUTION\n",
    "\n",
    "This notebook will give you solution code, and also point out how I want submitted code to look.\n",
    "\n",
    "Main point:  unless I explicitly ask for your code to execute something and produce results, your code should contain only your function definitions.  That is because to test your code I will put my test cases at the bottom of the notebook, then do \"Restart Kernel and Run All\" on the notebook, and expect to see the results of my tests (and nothing else!)\n",
    "\n",
    "Notice the difference between the Lab notebook and this notebook.  The Lab notebook is how you do development -- typically one function or other unit per cell, interleaved with testing.  This notebook is how you would submit a solution -- all functions in one cell, not testing or other output.\n",
    "\n",
    "You would typically delete this header cell from your handed-in solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Yourself\n",
    "* Intro Lab Solution\n",
    "* Steve Hanks\n",
    "* 1/3/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return contents of a file as a string\n",
    "# Input: Name of the file\n",
    "# Output:  A string containing the file contents\n",
    "\n",
    "def readFile(filename):\n",
    "    return open(filename).read()\n",
    "\n",
    "# Split a string into words on whitespace\n",
    "# Input: A single string\n",
    "# Output: A list of words (a word is a string that contains no whitespace)\n",
    "\n",
    "def splitIntoWords(str):\n",
    "    return str.split()\n",
    "\n",
    "# Tokenize a word -- remove non-letters and convert all letter to lower case\n",
    "# Input: a word\n",
    "# Output: the tokenized word\n",
    "\n",
    "import re\n",
    "def tokenizeWord(word):\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    return regex.sub('', word).lower()\n",
    "\n",
    "# Tokenize a file -- read the file, convert to tokens, remove tokens of length 0\n",
    "# Input: a file name\n",
    "# Output: a list of tokens\n",
    "\n",
    "def tokenizeFile(filename):\n",
    "    tokens = [tokenizeWord(word) for word in splitIntoWords(readFile(filename))]\n",
    "    return [tok for tok in tokens if len(tok) > 0]\n",
    "\n",
    "# Compute count per token\n",
    "# Input: a list of tokens\n",
    "# Output: a dictionary whose keys are tokens and whose values are counts of the number of that\n",
    "#           token in the input list\n",
    "\n",
    "from collections import Counter\n",
    "def countTokens(tokenList):\n",
    "    return dict(Counter(tokenList))\n",
    "\n",
    "# Most frequently occurring tokens in a file\n",
    "# Input:  a file name, limit on number of token counts to be returned\n",
    "# Output: a list of length at most N each of the form (token, count) in descending order of count\n",
    "\n",
    "def mostFrequentTokens(filename, n=10):\n",
    "    return(sorted(countTokens(tokenizeFile(filename)).items(), key=lambda x:x[1], reverse=True)[0:n])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
